# spbu_machine_learning_spring_2025
Практики по курсу "Машинное обучение" для программы "большие данные и распределенная цифровая платформа" ПМ-ПУ СПбГУ
Чтобы начать: делаете форк этого репозитория себе и отправляете мне в личку свои имя-фамилию и ссылку на реп.

# План курса
**План является предварительным и будет меняться**

1. Особенности линейной регрессии: 
   - 1 домашка. рекап (2 недели)
2. Регуляризация: done 
3. Метрические методы. kNN. PCA. Проклятие размерности. SVC 
   - 1 соревнование (лин.регрессия)
4. Работа с текстовыми данными. Bag-of-words, TF-IDF, Лемматизация.    (optional)****
   - Тест
5. Проблемы несбалансированных данных. Imblearn, метрики.:  
   - 2 соревнование (сентимент)
6. LOESS (локальные методы) / Калибрация моделей  
   - 2 домашка. решение задачи (3 недели)
7. Деревья решений 
8. Ансамбли 
   - 3 соревнование/задачи 
9. Кластеризация 
10. Снижение размерности: tsne,  umap 
  

# Домашние задания
В заданиях надо написать код, протестировать, проделать вычисления итп, на защите поговорить со мной.
В соревнованиях вы соревнуетесь друг с другои на Kaggle и, если мне станет интересно, со мной. 
Можно делать что угодно с учетом ограничений, которые будут указаны в тексте.
Дедлайн соревнований не привязан явно к мягкому или жесткому дедлайну, пока соренования будут длиться от 2 до 4 недель.

*Важно*: в начале курса убедитесь, что у вас есть аккаунт на kaggle, в противном случае я буду искать альтернативы.

## Список выданных заданий и сроков
1. Рекап. Выдан 22.02, мягкий дедлайн 8.03, жесткий 18.03. Защита по возможности после пары 8.03.

Алгоритмы с нуля. Ссылка: https://stepik.org/join-class/79ccadaa9bf79ec94ef7930910d380497f3c8e23. Дедлайн - зачет.


## Список группы, оценки

https://docs.google.com/spreadsheets/d/1gWB66KenLa5Oi-QUzxOOS9mB0-mbzZTKMucMbMwTkbU/edit?gid=0#gid=0


## Сроки

Мягкий дедлайн по каждому заданию - 14 день после его публикации, время защиты - ближайшая пара и далее. Если задание опубликовано в другой день, то время на выполнение - те же 14 дней, но защита будет после ближайшего занятия после.
Защититься можно и на других парах. Время защиты на оценку не влияет.
Примеры: 
- Задание  опубликовано в субботу до занятия или на нем, дедалайн - следующее занятие (последняя минута занятия). В конце следующего занятия происходит отсечка и начинается защита.
- Задание опубликовано в среду. Дедлайн на сдачу - среда через неделю (время пуша на гитхаб + 1 минута). Ждем до субботы, в субботу защита.

После мягкого дедлайна каждый день просрочки уменьшает оценку на 10 %. Итого жесткий дедлайн - 24 день. Защита также на ближайшей паре после него.
Расчет будет проходить следующим образом:
- Считается балл за задачи (как будто штрафа нет)
- Результат умножается на 1 - 0.1 * на количество просроченных дней
- Просрочка начинается, как только заканчивается время на сдачу. Т.е. если срок - 23.59 среды, в 00.00 четверга множитель уже 0.9.

Сдачей задачи считается коммит и пуш в именной репозиторий на гитхаб. Время коммита будет считаться временем сдачи.
Если с гитхабом проблемы, необходимо выслать файл с решением мне. В противном случае датой будет считаться дата защиты.
Можно делать сколько угодно коммитов до финальной сдачи. Я буду учитывать только последний их них. Исключение - исправление мелких ошибок и помарок в тексте, рефакторинг, итп. 

Для **соревнований**: дедлайн один (жесткий) - конец соревнования. После публикации результата необходимо прислать мне описание решения. Подойдет:
- Ноутбук с обучением модели + краткий отчет об идеях
- Набор ноутбуков с разными идеями 
- Набор .py файлов + краткий отчет об идеях

Также я планирую устраивать небольшие обсуждения идей на паре в процессе соревнования (и после).

**Исключение** для всех заданий - зачетный период, тогда срок будет min(дедлайн, зачет).

## Семинарские ноутбуки
За прорешенные семинарские ноутбуки будет добавляться (максимум) по 10 баллов. Они будут считаться особыми дополнительными заданиями.
Я выкладываю свое решение после занятия через одно. Их можно будет обсудить также, как домашку.
Таким образом, **дедлайн** по семинарам есть только один - жесткий, дается 2 недели. Защита по ним включает лишь показ кода. 


## Оценки и штрафы
Учет списывания будет вестись по коммитам. Кто первый закоммитил - тот и молодец. 

Неэффективный, плохо читаемый код или непонятные графики могут снизить оценку. 
Советую пользоваться линтерами (isort, black, pylint, mypy). 
Интересные находки - библиотеки для визуализации, работы с данными, отслеживанием экспериментов, продвинутые модели - все это может повысить оценку, даже немного сверх максимума.

## Защита заданий
На защите я буду задавать вопросы по работе - на понимание формул, кода - и другие уточняющие вопросы. Также я планирую прогонять тесты, так что лучше отправлять задачи до занятия, чтобы не тратить время.
До дедлайна решения можно будет исправлять, я буду присылать в личку информацию  о непройденных тестах или иных ошибках.
Если у вас были проблемы при выполнении задачи - при защите можно улучшить свою оценку, продемонстрировав понимание :) Или ухудшить, если окажется, что работа списана.
Если вы использовали какие-то источники, туториалы, видео итп при выполнении заданий, то мне нужно будет прислать отдельный файл с их списком.

# Финальная оценка
Оценка будет основана на баллах, заработанных в течение семестра. 
Баллы будут приведены в границы 0-1, где 1 соответствует максимальному числу базовых баллов.
Формула: $ 45 * домашки + 10 * классные + 5 * проверочная + 40 * соревнования $ 

Для получения зачета определены границы (в базовых баллах):

| e   | d   | c   | b   | a    |
|-----|-----|-----|-----|------|
| 60% | 70% | 80% | 90% | 100% |

Баллы с основных и дополнительных баллов суммируются. 

Получившие более 75% дополнительных баллов также освобождаются от задачи на экзамене. 
 
## Особенности линейной регрессии
Рассмотрены различные особенности данных, а также следующие положения линейной регрессии:
* Линейность: взаимосвязь между зависимыми и независимыми переменными линейна.
* Гомоскедастичность: дисперсия ошибок постоянна на всех уровнях независимых переменных.
* Нормальность: ошибки подчиняются нормальному распределению.
* Нет мультиколлинеарности: независимые переменные не сильно коррелируют друг с другом.
* Нет эндогенности: между ошибками и независимыми переменными нет связи.
* 
## Регуляризация
Рассмотрены L1, L2, Elastic-Net-регуляризация
  